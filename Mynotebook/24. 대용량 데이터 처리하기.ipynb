{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28e6e6d6",
   "metadata": {},
   "source": [
    "# 직접 해보세요!\n",
    "## 뉴욕 택시 데이터 준비(166쪽)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ad5b2",
   "metadata": {},
   "source": [
    "- 여러 개로 나누어진 데이터 불러오기\n",
    "- 데이터는 필요에 따라 나누어 저장하기도 한다. 데이터를 나누어 저장하면 용량이 작아져 데이터를 저장하거나 다른 사람에게 공유할 때 유용하다. 또 어떤 경우에는 처음부터 크기가 작은 데이터가 생성되는 경우도 있다.\n",
    "- 예를 들어 주식 정보를 매일 수집한다면 일 단위로 데이터가 생성된다. 지금까지는 한 덩어리의 데이터를 불러와 여러 가지 실습을 진행했었다.\n",
    "- 그러면 여러 개로 나누어진 데이터를 신속하게 불러오려면 어떻게 해야 할까"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbcdaf5",
   "metadata": {},
   "source": [
    "- 뉴욕 택시 데이터는 13억 대의 뉴욕 택시에 대한 정보를 가지고 있다. 파일의 개수도 140개나 된다. 이번 실습에서는 이 중 5개의 데이터만 사용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1bbd484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2015-01.csv\n",
      "\n",
      "../data\\fhv_tripdata_2015-01.csv\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2015-02.csv\n",
      "\n",
      "../data\\fhv_tripdata_2015-02.csv\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2015-03.csv\n",
      "\n",
      "../data\\fhv_tripdata_2015-03.csv\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2015-04.csv\n",
      "\n",
      "../data\\fhv_tripdata_2015-04.csv\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2015-05.csv\n",
      "\n",
      "../data\\fhv_tripdata_2015-05.csv\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import urllib.request\n",
    "\n",
    "# 네트워크 상태에 따라 5 ~ 15분이 소요됩니다.\n",
    "with open('../data/raw_data_urls.txt', 'r') as data_urls:\n",
    "    for line, url in enumerate(data_urls):\n",
    "        if line == 5:\n",
    "            break \n",
    "        fn = url.split('/')[-1].strip()\n",
    "        fp = os.path.join('', '../data', fn)\n",
    "        print(url)\n",
    "        print(fp)\n",
    "        urllib.request.urlretrieve(url, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb764e07",
   "metadata": {},
   "source": [
    "- 내려받은 데이터는 data폴더에 'fhv_tripdata_YYYY_MM.csv'라는 이름으로 저장된다.\n",
    "- 이제 판다스로 데이터를 불러오면 된다. glob 라이브러리에 포함된 glob 메서드는 특정한 패턴의 이름을 가진 파일을 한 번에 읽어 들일 수 있다.\n",
    "- glob 메서드로 5개의 파일을 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4d9a82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data\\\\fhv_tripdata_2015-01.csv', '../data\\\\fhv_tripdata_2015-02.csv', '../data\\\\fhv_tripdata_2015-03.csv', '../data\\\\fhv_tripdata_2015-04.csv', '../data\\\\fhv_tripdata_2015-05.csv']\n"
     ]
    }
   ],
   "source": [
    "import glob \n",
    "nyc_taxi_data = glob.glob('../data/fhv_*') \n",
    "print(nyc_taxi_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a0ff1d",
   "metadata": {},
   "source": [
    "- 그런 다음 각각의 파일을 데이터프레임으로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac48c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi1 = pd.read_csv(nyc_taxi_data[0]) \n",
    "taxi2 = pd.read_csv(nyc_taxi_data[1]) \n",
    "taxi3 = pd.read_csv(nyc_taxi_data[2]) \n",
    "taxi4 = pd.read_csv(nyc_taxi_data[3]) \n",
    "taxi5 = pd.read_csv(nyc_taxi_data[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b259d97",
   "metadata": {},
   "source": [
    "- 데이터를 잘 불러왔는지 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b744005",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00013  2015-01-01 00:30:00         NaN\n",
      "1               B00013  2015-01-01 01:22:00         NaN\n",
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00013  2015-02-01 00:00:00         NaN\n",
      "1               B00013  2015-02-01 00:01:00         NaN\n",
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00029  2015-03-01 00:02:00       213.0\n",
      "1               B00029  2015-03-01 00:03:00        51.0\n",
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00001  2015-04-01 04:30:00         NaN\n",
      "1               B00001  2015-04-01 06:00:00         NaN\n",
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00001  2015-05-01 04:30:00         NaN\n",
      "1               B00001  2015-05-01 05:00:00         NaN\n"
     ]
    }
   ],
   "source": [
    "print(taxi1.head(n=2)) \n",
    "print(taxi2.head(n=2)) \n",
    "print(taxi3.head(n=2)) \n",
    "print(taxi4.head(n=2)) \n",
    "print(taxi5.head(n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa6f3b3",
   "metadata": {},
   "source": [
    "- 각 데이터의 구조, 즉 행과 열을 확인해보자. 데이터가 꽤 크다는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "758b336f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2746033, 3)\n",
      "(3126401, 3)\n",
      "(3281427, 3)\n",
      "(3917789, 3)\n",
      "(4296067, 3)\n"
     ]
    }
   ],
   "source": [
    "print(taxi1.shape) \n",
    "print(taxi2.shape) \n",
    "print(taxi3.shape) \n",
    "print(taxi4.shape) \n",
    "print(taxi5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d62b540",
   "metadata": {},
   "source": [
    "- 이제 데이터 처리를 위해 각 데이터프레임을 연결해야 한다. 다음은 concat 메서드로 모든 데이터프레임(taxi1~5)을 연결한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f142d3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17367717, 3)\n"
     ]
    }
   ],
   "source": [
    "taxi = pd.concat([taxi1, taxi2, taxi3, taxi4, taxi5])\n",
    "\n",
    "print(taxi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2aab87",
   "metadata": {},
   "source": [
    "# 알아두면 좋아요!\n",
    "## 반복문으로 데이터 준비하기(169쪽)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45fe7ba",
   "metadata": {},
   "source": [
    "- 반복문을 응용하면 단 몇 줄의 코드로 데이터를 준비할 수 있다.\n",
    "- 앞에서 생성한 파일 목록(nyc_taxi_data)을 반복문으로 읽어 들인 다음 리스트(list_taxi_df)에 이어붙이면 된다. 그러면 리스트에 데이터프레임이 순서대로 저장된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13acc1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "list_taxi_df = [] \n",
    "\n",
    "for csv_filename in nyc_taxi_data:\n",
    "    # print(csv_filename)\n",
    "    df = pd.read_csv(csv_filename)\n",
    "    list_taxi_df.append(df) \n",
    "\n",
    "print(len(list_taxi_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb53e5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(list_taxi_df[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f65b94b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00013  2015-01-01 00:30:00         NaN\n",
      "1               B00013  2015-01-01 01:22:00         NaN\n",
      "2               B00013  2015-01-01 01:23:00         NaN\n",
      "3               B00013  2015-01-01 01:44:00         NaN\n",
      "4               B00013  2015-01-01 02:00:00         NaN\n"
     ]
    }
   ],
   "source": [
    "print(list_taxi_df[0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6249f77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17367717, 3)\n"
     ]
    }
   ],
   "source": [
    "taxi_loop_concat = pd.concat(list_taxi_df) \n",
    "print(taxi_loop_concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b734eca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(taxi.equals(taxi_loop_concat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662c74f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
